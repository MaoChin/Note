# `I/O`多路转接

`select`，`poll`，`epoll`都是==解决“等”的问题==。通过监视文件描述符的状态变化：可读，可写，异常(如连接关闭，文件描述符非法等)，等待事件就绪。

这些方案可以一次等待多个文件描述符。但是这些方案也是要等待的，所以也会涉及到等待策略的问题：阻塞式？轮询？信号驱动？？

所有的多路转接方案都涉及到两个步骤：

1. 用户告诉内核，需要检测哪些文件描述符的哪些事件。
2. 内核告诉用户，让我检测的文件描述符中哪些文件描述符的哪些事件就绪了。

## 1. `select`

```C
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set 		  			*exceptfds, struct timeval *timeout);
```

1. `nfds`：等待的文件描述符中最大的那个 再 +1。(文件描述符就是数组下标)
2. `fd_set`：位图结构，比特位的位置代表所有的文件描述符编号，0/1代表是否就绪。
3. `readfds`：输入输出型参数。输入：需要监视“读”事件就绪的文件描述符；输出：“读”事件已经就绪的文件描述符。
4. `writefds`：输入输出型参数。输入：需要监视“写”事件就绪的文件描述符；输出：“写”事件已经就绪的文件描述符。
5. `exceptfds`：输入输出型参数。输入：需要监视“异常”事件就绪的文件描述符；输出：“异常”事件已经就绪的文件描述符。这三个输入输出参数用的同一张位图，所以每次修改会覆盖！==所以每次调用`select`，都要重新设置这三个参数！！==
6. `timeval`：等待策略，设置`deadline`，在`deadline`内阻塞等待，超过`deadline`就`timeout`返回。设置成`nullptr`表示一直阻塞。
7. 返回值：>0 ：有几个`fd`就绪。

​					<0 ：出错

​					==0：`timeout`了

```C
// fd_set 的设置接口
// 用来清除描述词组set中相关fd 的位
void FD_CLR(int fd, fd_set *set); 
// 用来测试描述词组set中相关fd 的位是否为真
int FD_ISSET(int fd, fd_set *set); 
// 用来设置描述词组set中相关fd的位
void FD_SET(int fd, fd_set *set); 
// 用来清除描述词组set的全部位
void FD_ZERO(fd_set *set); 
```

### 1. `select`编码特点

1. **每次调用`select`之前都要重置那三个输入输出型参数**；**`select`调用完成之后要对所有合法的`fd`进行遍历检测**，确定是那些`fd`事件就绪。
2. ==需要用户自己维护一个数组来存放所有合法的`fd`==，这样`select`才能进行批量处理。
3. 一旦某个`fd`的事件就绪，那么==这次==对这个`fd`的操作不会被阻塞！

### 2. `select`的优缺点

优点：

1. 对比多进程/多线程，`select`占用资源少，更高效。 

缺点：

1. 每一次调用都要重置那三个输入输出型参数！！麻烦且影响性能。
2. ==能够同时检测的`fd`数量有限==！！(`fd_set`位图是128字节，也就是只有128*8=1024个比特位，即最多只能同时检测1024个`fd`！！)
3. 每一次调用都需要从用户到内核，内核到用户传递参数。这中间的数据拷贝会影响性能。(这是多路转接都有的问题！)
4. ==每一次调用前和调用后都要遍历自己维护的合法`fd`数组，检测所有的`fd`==，调用前是为了重置参数，调用后是为了判断是哪一个`fd`就绪，效率低下！

## 2. `poll`

```C
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

// pollfd结构
struct pollfd {
  int fd; 				// file descriptor 
  // 输入输出分离
  short events; 	// requested events  用户->内核
  short revents; 	// returned events 	 内核->用户
};
```

1. `fds`：`pollfd`的数组，也就是结构体数组。
2. `nfds`：`fds`数组的长度(元素个数)。
3. `timeout`：单位是毫秒，`timeout`时间内阻塞等待，0表示非阻塞，-1并表示一直阻塞等待。和`select`几乎一样。
4. 返回值：和`select`一模一样。
5. `events`：`short`2个字节，16比特位，每一位代表不同的事件。比如读事件，写事件，各种异常事件等。通过比特位传递信息，也是位图。

### 1. `poll`编码特点

1. 将输入和输出分开：`event`，`revent`。不用每一次调用前对参数进行重新设置！
2. **`poll`调用完成之后要对所有合法的`fd`进行遍历检测**，确定是那些`fd`事件就绪。
3. ==需要用户自己维护一个结构体数组来存放所有合法的`pollfd`结构体==。

### 2. `poll`的优缺点

优点：

1. 不用每一次调用前对参数进行重新设置！
2. **同时检测的`fd`数量可以非常多**，因为是用户自己维护的`pollfd`结构体数组。

缺点：

1. 还是需要用户自己维护`pollfd`结构体数组。
2. 每一次调用都需要从用户到内核，内核到用户传递参数。这中间的数据拷贝会影响性能。(这是多路转接都有的问题！)
3. ==每一次调用后都要遍历自己维护的合法`pollfd`结构体数组，检测所有的`fd`==，判断是哪一个`fd`就绪，效率低下！

## 3. `epoll`

**一个完美的解决方案！！**一样的，只负责"等"！

```C
// epoll相关系统调用
// 1.创建一个epoll的句柄
int epoll_create(int size);
// 2.epoll的事件注册函数   用户 -> 内核
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
// 3.收集在epoll监控的事件中已经发生的事件  内核 -> 用户
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, 									int timeout);

// struct epoll_event
typedef union epoll_data {
  void    *ptr;
  int      fd;
  uint32_t u32;
  uint64_t u64;
} epoll_data_t;

struct epoll_event {
  uint32_t     events;    /* Epoll events */
  epoll_data_t data;      /* User data variable */
};
```

### 1. `epoll_create`(创建`epoll`模型) 

1. `size`现在已经被忽略了！直接设置成256(或者其他大于0的数)就可以了。
2. 返回值：一个文件描述符。可以理解为一个`epoll`模型。

### 2. `epoll_ctl`(==用户 -> 内核==)

这个函数就是负责把需要检测的文件描述符设置到`epoll`模型中，或者从`epoll`模型中删除掉对某些文件描述符的检测等。

1. `epfd`：`epoll_create`返回的那个文件描述符，指明要操作哪一个`epoll`模型。
2. `op`：操作方法，比如`EPOLL_CTL_ADD`，`EPOLL_CTL_MOD`，`EPOLL_CTL_DEL`。
3. `fd`：需要检测的文件描述符。
4. `event`：`fd`需要检测的事件。和`poll`一样，也是位图结构，每一个比特位代表不同的事件。

### 3. `epoll_wait`(==内核 -> 用户==)

这个函数是负责从内核获取事件就绪信息。

1. `epfd`：`epoll_create`返回的那个文件描述符，指明要操作哪一个`epoll`模型。

2. `events`：输出型参数，是一个数组，表示需要检测的事件中已经就绪的事件列表。`event`里面有`fd`信息！！

3. `maxevents`：`events`的长度。

4. `timeout`：和`poll`一模一样。

5. 返回值：和`select`一模一样。


### 4. `epoll`底层原理

==`epoll`模型：回调机制+红黑树+就绪队列！！==这些都是内核做的，不需要用户关心！

当数据到达网卡后(读事件就绪)，**在硬件上会触发中断**，通知`CPU`有数据到来，然后`CPU`会通知`OS`，最后`OS`会调用内核的拷贝函数把网卡的数据拷贝到内核缓冲区，在这个拷贝函数中可以==设置回调！==

`epoll`针对一个或多个特定的`fd`会==设定对应的回调机制==，当`fd`缓冲区中有数据的时候会进行回调，也就是调用回调函数。回调函数主要处理以下事情：

1. 获取就绪的`fd`。
2. 获取就绪事件，检测该事件是不是用户让我关心的。
3. 构建`queue_node`节点并链入到就绪队列里。

另外，`epoll`模型中还会维护一棵红黑树：存放 ==用户 -> 内核 的信息==！

```C
// 红黑树节点  fd天然就可以作为红黑树的key !!!
struct rb_node
{
  // 表示用户关心的哪一个文件描述符的哪些事件
  int fd;
  int events;
  // ...
};
```

**所以`epoll_ctl`就是对红黑树进行增删查改！**

另外，`epoll`模型中还有一个就绪队列： 存放 ==内核 -> 用户 的信息==！

```C
// 就绪队列节点
struct queue_node
{
   // 表示红黑树中哪些节点的哪些事件已经就绪！
  int fd;
  int revents;
  // ...
};
```

**`epoll_wait`就是去检测就绪队列，有数据的话就把就绪队列的数据拿出来！！**检测的时间复杂度是O(1)的！而且内核往就绪队列里放数据，用户从就绪队列里取数据---生产者消费者模型！！！	

回想`TCP`协议中的`PSH`标志位，本质就是再往这个就绪队列里放数据，通知用户赶快读取。

### 5. `epoll`的工作模式：`LT && ET`

`select`和`poll`都是`LT`工作模式，`epoll`默认情况下也是`LT`工作模式，但是`epoll`用户可以自己设置成`ET`工作模式。

#### 1. `LT(Level Triggered)`

只要底层有数据就会==一直通知==上层读取数据。**这样我们上层编码的时候底层有数据到来我们可以只读取一部分或者直接不读取**，因为底层会一直通知我们的，不用担心数据丢失的问题。

底层通知上层：就是上层调用`epoll_wait`时可以获取到对应文件描述符的事件就绪。

#### 2. `ET(Edge Triggered)`

只有底层数据==变化的的时候==(从无到有，从有到多)才会通知上层读取数据。**这样就倒逼我们在底层数据到来时必须把数据全部读取完**！！不然后面底层就不会通知上层了，直到这个`fd`的数据再次有变化，那万一后面再也没有数据变化了呢？？之前没有读取的数据就丢失了！

理论上说，`ET`比`LT`更高效，因为`LT`模式底层通知上层时会有大量重复的历史通知！这个通知也是有开销的。而且：`LT`模式下上层不及时取走数据会导致底层缓冲区积压，导致`TCP`报头的窗口变小，**网络传输变得低效！！**

但是`LT`模式下每次底层通知上层时上层就会把数据全部读取完，那就和`ET`一样！！只不过==`ET`是一种倒逼程序员尽快取走数据的手段。==

#### 3. 上层如何判断底层的数据被读取完了？

只能死循环读取！！直到读取到的数据小于预期或者读取不到数据了(最后要多读一次)。但是`fd`有可能是阻塞式的，==所以最后一次读取就会导致`read/recv`阻塞住！==

所以在`ET`模式下，==所有的`fd/sockfd`必须是非阻塞式的！！(**面试题**)==  而且建议`LT`模式下也将`fd/sockfd`设置成非阻塞。(系统调用`fcntl`)

```C
int fcntl(int fd, int cmd, ... /* arg */ );

void SetNoBlock(int fd) {
  int fl = fcntl(fd, F_GETFL);
  if (fl < 0) {
    perror("fcntl");
    return;
  }
  fcntl(fd, F_SETFL, fl | O_NONBLOCK);
}
```



### 6. `epoll`的优缺点

 优点：

1. 需要添加检测`fd`时直接调用`epoll_ctl`，获取`fd`状态信息直接调用`epoll_wait`，==不用遍历数组==！！底层是红黑树和就绪队列。
2. ==不用自己维护`fd`数组。==

缺点：

1. 每一次调用都需要从用户到内核，内核到用户传递参数。这中间的数据拷贝会影响性能。(这是多路转接都有的问题！)



