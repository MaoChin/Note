# `Redis`集群

提高总的`redis`服务的存储容量。

广义的集群：只要有多台机器构成了分布式系统，就可以认为是一个集群，在这个概念中，主从结构和哨兵机制都可以看成一个集群。

`redis`的集群：`redis`提供的集群模式，主要是为了解决存储空间不足的问题。

## 1. 基本概念

`redis`哨兵只是提高了可用性，但是**所有的数据**还是要存储在主节点和从节点中(内存中！！)，当数据量非常大时，会导致存储空间不足的问题。(内存毕竟是有限的)

==集群就是引入多组 `Master/Slave`，每一组存储所有数据的一部分即可。==每一组`Master/Slave`叫做一个分片(`Sharding`)。

**每一组`Master/Slave`中的所有机器存储的数据是相同的，不同的`Master/Slave`之间存储的数据不同！！**

## 2. 数据分片算法(面试题)

就是如何把一大份数据分成若干小份？？(==重点==)

### 1. 哈希求余

针对给定的`key`，先进行哈希算法(如MD5)得到`hash`值，再进行 ==模 N 求余==。(N 是分片个数)

```shell 
hash(key) % N
```

优点：简单高效， 数据分配较为均匀。

缺点：当进行**扩容时数据搬运量非常大**。就是 N 改变时，数据分片的结果会有很大变化，这就需要在多个`Master/Slave`之间进行数据传输，以适配新的分片。网络开销很大！！所以生产环境中一般不用。

**MD5：使用很广泛的哈希算法，主要有三大优点：**

1. 结果定长。
2. 结果分散，就是原字符串只要有一点不同，结果也会有很大不同。
3. 不可逆，基本不可能从结果恢复出原字符串。

### 2. 一致性哈希

简单来说就是==分段==，把 哈希值的范围 分成 N 个区间，然后对所有的 `key`计算哈希值，哈希值在哪个区间就对应那个下标。这样在扩容时只需多划分一个区间即可，要搬运的数据只有一小部分。

优点：降低了扩容时数据搬运的规模，提高了扩容操作的效率。

缺点：**数据分配不均匀**，有的分区数据多，有的分区数据少。

### 3. 哈希槽分区(`redis`使用的)

==本质就是把 哈希求余 和 一致性哈希算法 这两种做法相结合。==

先根据`key`计算出其”槽位“：==key -> 槽位 -> 分片==

```shell
# crc也是一种哈希算法，和 MD5 差不多
# 一共有 16384(1024*16) 个槽位
hash_slot = crc16(key) % 16384
```

然后**一个分片对应若干个槽位 上的`key`即可。**

假设当前有三个分片,一种可能的分配方式：
  		1.  0号分片：[0,5461]，共5462个槽位
  		2.  1号分片：[5462,10923]，共5462个槽位
  		3.  2号分片：[10924,16383]，共5460个槽位  

每个分片对应的槽位可以不连续，数量差不多就可以了。每个分片都用位图(2KB)记录他对应的槽位。

扩容时就取出每个分片中一定数量槽位的数据给新的分片，保持每个分片的槽位数差不多。这样要搬运的数据也不是很多。

==既保证了每个分片上数据量差不多，又使得扩容时的效率比较高。==

两点说明：

1. 分片数一般不要太多，不要超过1000。分片太多难以保证数据均匀分配，而且太多分片的集群本身的可用性是难以保证的(出现故障的可能性非常大)。
2. 为什么是16384个槽位：分片中记录槽位的位图是包括在心跳包中的，也就是要频繁进行网络传输，所以位图应尽可能的小。而槽位太少也不好，会导致每个槽位的`key`变多，影响性能。所以这是个折中的取值。

## 3. 基于`docker`搭建`redis`集群

和哨兵模式那里一样，在实际中每一个节点都是部署在不同的服务器上的，这样才有集群的意义！！！这里只是为了学习演示，看看效果。

### 1. 认识`shell`脚本(`xxx.sh`)

把多条`shell`命令写到一个文件里，批量化执行。并且还支持分支，循环，函数等，这样就可以进行比较复杂的命令编写了。

**对于多个类似的操作可以编写脚本去完成，这样更方便快捷，并且可以重复使用。**

`shell`脚本要求把所有的代码写到一行，但是这样不方便看，所以使用很多续行符 \ 。

```bash 
# seq是shell命令，表示生成 [1,9]
for port in $(seq 1 9); \ # 这个 \ 是续行，就是下一行的内容和这一行合并。
do \                      # do .... done 表示代码块
mkdir -p redis${port}/    # shell中 {} 中的是变量
touch redis${port}/redis.conf
cat << EOF > redis${port}/redis.conf
port 6379
bind 0.0.0.0
protected-mode no
appendonly yes                         # 开启AOF持久化
cluster-enabled yes                    # 开启集群
cluster-config-file nodes.conf         # 集群配置文件,自动生成
cluster-node-timeout 5000              # 集群间心跳包超时时间
cluster-announce-ip 172.30.0.10${port} # 节点ip(容器内)
cluster-announce-port 6379           # 节点port(容器内)，业务端口,提供服务
cluster-announce-bus-port 16379      # 管理端口,完成一些管理上的任务
EOF
done

# 注意 cluster-announce-ip 的值有变化.
for port in $(seq 10 11); \
do \
mkdir -p redis${port}/
touch redis${port}/redis.conf
cat << EOF > redis${port}/redis.conf
port 6379
bind 0.0.0.0
protected-mode no
appendonly yes
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
cluster-announce-ip 172.30.0.1${port}
cluster-announce-port 6379
cluster-announce-bus-port 16379
EOF
done
```

执行脚本：`bash xxx.sh`

### 2.` docker-compose.yml`

==注意网络配置。网段的划分要是内网`ip`：==

1. **10.***
2. **172.16.* ~ 172.31.***
3. **192.168.***

### 3. 启动容器与构建集群

```shell
docker-compose up -d  # -d 表示后台启动
```

```shell
redis-cli --cluster create 172.30.0.101:6379 ...(所有的ip:port) 
	--cluster-replicas 2 (表示一个主节点配两个从节点)
```

```shell
# 这些节点构成了一个集群，所以连接不同的节点本质上是一样的
# -c：有可能设置/获取的key不在这个连接的分片上！！而在其他的分片上，-c 选项就可以自动把请求重定向到对应的分片上。
redis-cli -h 172.30.0.101 -p 6379 -c
cluster nodes  # 查看集群信息
```

集群中某个主节点宕机之后的**处理结果**和前面的哨兵模式是一样的，都是挑选一个从节点上位，担任主节点。但是具体的实现细节和哨兵模式不一样：

**集群中没有哨兵节点一说，而是集群中的每一个节点都承担了哨兵的任务！！**大家互相监控。发现某个主节点宕机后其所属的从节点进行短暂的休眠后会进行拉票，但是只有主节点可以投票，先休眠结束的那个从节点一般就会晋升为主节点，自己执行`slave no one`，然后让其他的从节点直行`slave ip port`认主。

以下三种情况会被视为==整个集群挂了==：

1. 某个分片的所有主从节点全部宕机。
2. 某个分片的主节点挂了，但是没有从节点。
3. 超过一半的主节点挂了。

### 4. 集群扩容与缩容

扩容就是引入更多的机器，存储更多的数据。

==集群扩容的风险较高，成本也较大！！所以一般不会频繁扩容。==

#### 1. 添加新的主节点

```shell
 redis-cli --cluster add-node 172.30.0.110:6379 172.30.0.101:6379
```

#### 2. 重新分配`slots`

```shell
redis-cli --cluster reshard 172.30.0.101:6379  # 集群中的任意节点ip:port
```

#### 3. 给新的主节点添加从节点

```shell
redis-cli --cluster add-node 172.30.0.111:6379 172.30.0.101:6379 --cluster-slave
```









